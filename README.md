# ğŸš€ Loan Approval Prediction

Predict whether a loan application should be approved using the classic Loan Prediction dataset. This repository provides modular notebooks, reusable Python modules, and reproducible artifacts for data exploration, model building, tuning, and deployment.

---

## ğŸ† Key Features

- **Fast Data Ingestion & Validation:**  
  Utilities for loading and validating data (`src/data_collection.py`, `src/artifacts.py`).

- **Modular Preprocessing:**  
  Handles missing values, categorical encoding, and train/test splits (`src/preprocessing.py`).

- **Exploratory Data Analysis:**  
  Univariate and bivariate analysis notebooks (`notebooks/01â€“04_*`).

- **Baseline Model Comparison:**  
  Evaluate seven classifiers with metrics and confusion matrices (`src/model_baseline_and_evaluation.py`).

- **Automated Hyperparameter Tuning:**  
  For Logistic Regression, Linear SVM, and Naive Bayes, with persisted model artifacts (`src/tuner.py`).

- **Deployment Helper:**  
  Aligns incoming payloads with the training schema before scoring (`src/model_deployment.py`).

- **Batch Notebook Execution:**  
  Automate the pipeline with Papermill (`src/run_notebooks.py`).

---

## ğŸ“ Repository Structure

```text
Loan-Approval-Prediction/
â”œâ”€ data/
â”‚  â””â”€ raw/
â”‚     â””â”€ loan_data_set.csv         # Original Kaggle-style dataset
â”œâ”€ artifacts/                      # Created after running notebooks/scripts
â”‚  â”œâ”€ 01_raw.parquet               # Cached raw dataframe
â”‚  â”œâ”€ 02_clean.parquet             # Cleaned + encoded dataframe
â”‚  â”œâ”€ expected_cols.pkl            # Training feature schema
â”‚  â”œâ”€ tuned_model_summary.csv      # Hyperparameter tuning report
â”‚  â””â”€ *_best.pkl                   # Persisted best-performing estimator
â”œâ”€ notebooks/                      # Jupyter notebooks for each pipeline stage
â”œâ”€ runs/                           # Papermill outputs (ignored until generated)
â”œâ”€ src/
â”‚  â”œâ”€ config.py                    # Centralized paths and constants
â”‚  â”œâ”€ data_collection.py           # CSV loading + preview helpers
â”‚  â”œâ”€ preprocessing.py             # Cleaning, encoding, split utilities
â”‚  â”œâ”€ eda_univariate.py            # Target distribution & data quality checks
â”‚  â”œâ”€ eda_bivariate.py             # Pairwise visualizations and insights
â”‚  â”œâ”€ feature_selection.py         # SelectKBest, chiÂ², ANOVA feature ranking
â”‚  â”œâ”€ model_baseline_and_evaluation.py
â”‚  â”œâ”€ tuner.py                     # GridSearchCV with persistence
â”‚  â”œâ”€ model_deployment.py          # Inference wrapper
â”‚  â””â”€ run_notebooks.py             # Papermill automation
â”œâ”€ requirements.txt                # Dependency list
â””â”€ README.md                       # Project documentation
```

---

## ğŸ“Š Dataset

- **File:** `data/raw/loan_data_set.csv`
- **Rows/Columns:** 614 rows, 13 columns (Loan_ID, demographics, credit history, loan details, target)
- **Target:** `Loan_Status` (converted to `Loan_Status_Y` after dummy encoding)
- **Source:** Kaggle â€œLoan Prediction Problemâ€ dataset  
  *(Update `config.CSV_PATH` if you use your own data)*

---

## âš™ï¸ Environment Setup

> **Note:** The current `requirements.txt` is a placeholder. Install dependencies manually or update the file before sharing.

1. **Install Python 3.10+** (Anaconda recommended)
2. **Create and activate a virtual environment:**
    ```sh
    conda create -n loan-approval python=3.10
    conda activate loan-approval
    ```
3. **Install core packages:**
    ```sh
    pip install -U pandas numpy scikit-learn matplotlib seaborn joblib papermill jupyter ipykernel
    ```
4. **(Optional) For Parquet support:**
    ```sh
    pip install pyarrow
    ```
5. **(Optional) Save dependencies:**
    ```sh
    pip freeze > requirements.txt
    ```

---

## ğŸš¦ Running the Workflow

### 1. Interactive Notebooks

Launch Jupyter Lab/Notebook and open the files under `notebooks/`:

- `01_data_collection.ipynb` â€“ Inspect raw data and schema
- `02_preprocessing.ipynb` â€“ Clean, encode, and split
- `03_eda_univariate_bivariate.ipynb` â€“ Exploratory analysis
- `04_feature_selection.ipynb` â€“ Feature scoring
- `05_model_baseline_and_evaluation.ipynb` â€“ Compare baseline models
- `06_final_model.ipynb` â€“ Hyperparameter tuning and artifact export
- `07_model_deployment.ipynb` â€“ Inference walkthrough

Each notebook saves intermediates in `artifacts/` for reproducibility.

---

### 2. Command-Line Scripts

All reusable functions live under `src/`. Example usage:

```python
import pandas as pd
from src.config import CSV_PATH
from src.preprocessing import clean_data
from src.model_baseline_and_evaluation import run_baseline_and_evaluation
from src.tuner import run_tuning_and_evaluation

# Load + clean
df_raw = pd.read_csv(CSV_PATH)
df_clean = clean_data(df_raw)

# Baseline comparison
baseline_results = run_baseline_and_evaluation(df_clean, target="Loan_Status_Y")

# Hyperparameter tuning + artifact persistence
tuned_results, best_model, model_path = run_tuning_and_evaluation(df_clean, target="Loan_Status_Y")
print(tuned_results)
print(f"Best model saved to: {model_path}")
```

---

### 3. Batch Notebook Execution

For automated runs (e.g., scheduled retraining), use Papermill:

```sh
python -m src.run_notebooks
```
> Ensure the `aiml` IPython kernel referenced in `run_notebooks.py` exists, or adjust `kernel_name`.

---

## ğŸš€ Model Deployment

After tuning, use the generated `*_best.pkl` and `expected_cols.pkl` in `artifacts/` to score new applications:

```python
from src.model_deployment import predict

sample = {
    "Gender": "Male",
    "Married": "Yes",
    "Dependents": "1",
    "Education": "Graduate",
    "Self_Employed": "No",
    "ApplicantIncome": 5000,
    "CoapplicantIncome": 1500,
    "LoanAmount": 120,
    "Loan_Amount_Term": 360,
    "Credit_History": 1.0,
    "Property_Area": "Urban",
}
preds, probs = predict([sample])
print(preds, probs)
```
> `predict` returns the hard class (0/1) and, when available, the approval probability.

---

## ğŸ“¦ Artifacts and Outputs

- `artifacts/01_raw.parquet`, `artifacts/02_*` â€“ Cached datasets
- `artifacts/*_best.pkl` â€“ Tuned estimator
- `artifacts/expected_cols.pkl` â€“ Training columns for inference
- `artifacts/tuned_model_summary.csv` â€“ Tuning results

Delete the `artifacts/` folder to start fresh.

---

## ğŸ› ï¸ Troubleshooting

- **Non-ASCII characters:**  
  If you see `ï¿½` in output, switch to UTF-8 (`chcp 65001`) or use WSL/Git Bash.
- **Papermill kernel issues:**  
  Install with `python -m ipykernel install --user --name aiml`.
- **Unexpected columns:**  
  `clean_data` re-runs dummy encoding; review if columns look wrong.

---

## ğŸš§ Next Steps

- Replace the placeholder `requirements.txt` with actual dependencies.
- Add automated tests for preprocessing and inference.
- Track experiment metrics with MLflow or Weights & Biases.
- Containerize the deployment helper (FastAPI/Flask + Docker).

---

Happy modeling!  
Open an issue or reach out if you need help extending the workflow.